\chapter{Hardware Design}
\label{ch:hw_design}
\section{Core Design}
We decided to design and implement two unique RocketCore CPUs for the heterogeneous SoC. These will be in two separate tiles with cache coherency. The aim will be for each core to implement the RV64GC ISA, the 'general-purpose' RISC-V ISA, as well as an MMU (Memory Management Unit) in order to run a full Debian Linux OS, if only with terminal interaction. The following code snippets show the parameterisation options available for the RocketCore CPU and the tiles they lie within.

\begin{figure}
    \includegraphics[]{./img/core_params.png}
\end{figure}
\begin{figure}
    \includegraphics[]{./img/rocket_tile_params.png}
\end{figure}

The micro-architecture specification of RocketCore is not publicised. This means implementation details of the core are not documented, and makes understanding how the core functions a difficult task. For example, to identify what changes occur when \texttt{useVM} is enabled instead of disabled, we must search through the source code (10000+ lines of Chisel\cite{}) to find where the value is used and what it is used for - what other variables are impacted, how they change, etc. The following sections identify and discuss what customisation is available, with parameters common between all cores specified and parameters that will be varied during testing identified.
%todo cite above
\subsection{Constant elements in RocketCore}
\subsubsection{Core Frequency - \texttt{bootFreqHz}}
The \texttt{bootFreqHz} core variable does not impact the actual frequency of the core, but instead is added to the device tree structure, so that an operating system is able to read the boot frequency of the CPU. The actual frequency of the cores are defined by clocks generated in the Vivado project. These can be edited, with supported frequencies of 160, 125, 100, 80, 62.5, 50, 40, 31.25, 25, and 20 MHz. The clock manager uses phase locked loop and counters to divide the central clock from the FPGA crystal oscillator. This reduces the available frequencies to those that can be formed from that central clock using the dividers. The Nexys-A7-100t has a crystal capable of generating up to 50 MHz, and is shared between all cores in the RocketChip SoC. This prevents individual clock frequencies for cores, one of the significant changes between big and small cores in typical heterogeneous systems. This will severely limit the actual performance difference between cores, as well as the power consumption difference. This also prevents frequency scaling as the RocketChip SoC does not have control of the clock generator, another key part of modern power saving measures. These are limitations of FPGAs as a platform and the SoC generators used in this project.

\begin{figure}
    \includegraphics[]{./img/single_clock_rocketchip.png}
\end{figure}

\subsubsection{ALU}
The integer ALU is not customisable within RocketCore. This results in all RocketCore CPUs having the same integer additions per cycle, and as we cannot vary the frequency between cores in the FPGA, all our designs will have the same integer operations per second.

\subsubsection{}

\subsection{MMU}
The \texttt{useVM} option for core parameters controls whether an MMU (Memory Management Unit) is instantiated inside the core and virtual memory is used. MMU is enables the use of Virtual Memory, an abstraction of physical addresses to logical addresses. This increases the security and stability of a system, by preventing processes from accessing the memory space of other processes. Stopping reads to another processes' memory space ensures sensitive data currently held in memory by a process cannot be accessed by another process. Stopping writes to the memory space prevents the corruption of another processes' data, that could otherwise lead to user data loss, the process stopping unexpectedly or entire system crash depending on the importance of the memory corrupted process. Virtual memory can also allow more logical memory than there is physical memory to be allocated to programs.

The MMU in RocketCore is non-parameterisable - it is either instantiated inside the CPU or it isn't. We have chosen to enable virtual memory and have an MMU instantiated inside the cores. Support for the Linux kernel and Debian requires an MMU, hence it is needed in the cores. %todo include LUT comparison with enabled/disabled

\subsection{User/Supervisor modes}
The \texttt{useUser} and \texttt{useSupervisor} options control the addition of hardware privilege levels. All RISC-V CPUs have the machine privilege level (M-mode). Code run in M-mode is able to execute any instruction, including those that would result in the CPU becoming trapped permanently. M-mode is intended to be used for managing secure execution done in lower privilege levels, completely trusted code that must be run on M-mode for system setup or for embedded systems where M-mode is the only privilege level implemented.

User mode (U-mode) can be added to the CPU to provide a secure environment where code can be executed safely, and wouldn't be able to cause serious damage to the system. The CPU keeps track of state at prevents this, but current privilege level is not visible to software as this would become a virtualisation hole - a way for a program to attempt to escape or see outside it's current virtualisation level. Privileged instructions cannot be run in U-mode, preventing access to registers such as \texttt{mtvec}, keeping the address of the machine trap vector and mode. U-mode is typically implemented for secure embedded systems, where a level of privilege is required to ensure the system does not fail when application code is run.

Supervisor mode (S-mode) can also be added. This privilege level fits between U-mode and M-mode, adding many S-mode registers that only M-mode had previously, such as \texttt{SPP} for the previous privilege mode or \texttt{stvec} for the supervisor trap vector address and mode. S-mode is utilised most often by OSs that want to be able to run applications on top of themselves and keep functioning, so must prevent such applications from being able to maliciously or accidentally interfere with it's operation.

As the SoC designed by this project will never be put into a production environment, we can disable user and supervisor mode options. However a requirement for virtual memory and implementing the MMU in RocketCore is the inclusion of supervisor mode, and so the the extensions will be implemented regardless of the options. While the U and S-modes are implemented, we have chosen to execute all code in M-mode. This removes hardware security from the system, but the only code being run will be either written by the author or from trusted open-source projects. In addition to this, the system is completely isolated and would be unable to cause any damage if compromised, only connected to another system via serial. There are also sections of the code that must be run in M-mode, like fetching from performance monitoring registers, and it is easier to execute fully in M-mode than to write traps and handlers in assembly to create M-mode code to fetch data from the registers separately.

\subsection{Hypervisor mode}
\texttt{useHypervisor} option enables the hypervisor mode extension. Hypervisor mode extends supervisor mode, adding additional virtualisation. S-mode becomes HS-mode, where a hypervisor program or hosting-capable OS runs, and changes some registers typically accessed from \texttt{Sxxx} to \texttt{Hxxx}, such as \texttt{SIE} to \texttt{HIE}. A virtualisation bit is added and set when the CPU is executing a guest OS, and this changes the HS-mode and U-mode to VS-mode and VU-mode. An additional layer of address translation is enabled when this occurs, as well as the registers accessed in VS-mode returning to the \texttt{Sxxx} versions.

This mode is used for systems where multiple OSs may be running concurrently, such as servers or workstation PCs. This project does not aim to design an SoC to be used in this context and we can safely disable this option.

\subsection{Debug mode}
Debug mode has been preliminarily proposed as an extension to the RISC-V ISA. The \texttt{useDebug} option controls the implementation of this extension. When enabled, this generates hardware in the core to enable debugger control of the core, as well as a debugger module in the SoC. The changes to the core include addition of CSR registers, writable only by an external debugger interacting with the debug module, that can halt execution, force jumps, and other control functions. The debug module allows an external debugger to get the contents of core data registers, cache, RAM, and CSRs as needed, enabling someone testing hardware to inspect the status of the core during program execution. The debug module also provides the capability to 'step through' the instructions, a very useful feature when identifying the exact instruction where a program deviates from expected execution.

We have enabled this option during the design phase, as the debug features it provides is very useful when designing the test software. For the actual core under test the option was disabled, to reduce the FPGA usage for comparisons and maximise the available resources for additional processing hardware.

\subsection{Atomics}
The \texttt{useAtomics} and \texttt{useAtomicsOnlyForIO} options enable parts of the Atomic RISC-V extension. The atomic extension implements instructions allowing for read-modify-write to memory and IO while keeping memory consistency between multiple RISC-V harts utilising the same memory/IO.

Base RISC-V ISA has a relaxed memory model, allowing loads and stores to take place in any order if unspecified. The atomic extension implements load-reserved and store-conditional instructions that can apply additional ordering constraints on memory/IO operations, and ensure data is valid during operations performed on it. There are two bits used to specify acquire and release ordering requirements for this RISC-V hart: when the acquire bit is set, no memory operation instructions following the acquire-set instruction can take place before the acquire-set operation; when the release bit is set, the release-set instruction must take place after any previously issued memory operations. The combination of these bits allow for sequential ordering of memory operations for a RISC-V hart can be asserted. These instructions are primarily utilised for out-of-order CPUs, where the ordering of issued instructions does not necessarily match the order of instruction execution, leading to race conditions without atomically defined instructions.

The atomics also implement instructions for multicore synchronisation. Atomic write-modify instructions for memory perform changes that occur uninterrupted with a single instruction. These also implement the acquire and release bits, and allow for swapping, addition, bitwise operations and maximum/minimum instructions to be completed on data held in shared memory without other harts interefering.

As the system we are designing is multicore, we enabled the \texttt{useAtomics} option. Not allowing simultaneous memory access in a multicore system would cause a severe bottleneck in most cases and is the only other solution to ensure memory validity in this case.

\subsection{Compressed instructions}
Enabling the compressed instruction set extension is enabled using the \texttt{useCompressed} option. The extension adds support for 16-bit versions of common RISC-V instructions and are utilised whenever possible to reduce the code-size when compiled. In order to use 16-bit instructions instead of 32-bit, the data embedded in the instruction must be able to be stored in a reduced amount of bits than provided in 32-bit: for instance, when performing integer additions where a source and destination register are specified, if the registers are the same then the instruction can likely be compressed. The compressed extension affects the base ISA (RV64I) as well as the single-precision and double-precision extensions, though this must be supported in the implementation of these instead of the compressed extension.

With compressed enabled, the code-size can typically be reduced by 25-30\%, with 50-60\% of 32-bit instructions being replaced with 16-bit counterparts. This is a huge reduction, and is especially useful in embedded systems where storage space can be extremely limited. We enabled this option to add compressed instruction compatibility to the design, and allowing the code-size reductions that it provides.

\subsection{Bit manipulation and Vector}
\texttt{useBitManip} and \texttt{useVector} options exist in the \texttt{CoreParams} class used for defining core designs. However, RocketCore is not advertised as implementing these extensions and so their function was unknown. Examining the RocketChip code that utilises these options reveals a partial implementation of the vector extension, where CSR registers are added for control, as well as logic to update dirty bits and other status bits. This is the only vector code implemented however - there are no registers or other logic defined for the vector extension. Instructions have been defined in Chisel for the vector operations, but there is no decode logic defined for the instructions and so they cannot be used. The \texttt{useBitManip} option only adds the bit manipulation extension letter to the ISA string held in the CSR, making no other changes. As such, these options have been left disabled in the design as they serve no functional purpose.

\subsection{SCIE}
Custom instructions support is enabled within RocketCore by enabling the \texttt{useSCIE} option (Si-Fice Custom Instruction Extension). This enables support for custom instructions, producing a custom instruction logic interface. Connected custom instruction logic is supplied with the current instruction, data from two registers and can output register data and a dispatch code. As we are not writing any custom instructions for this project, the extension is unnecessary and we have not enabled it in the designs.

\subsection{RVE}
\texttt{useRVE} changes the base ISA to RV32E instead of RV32I when enabled. RV32E supports the same instructions and extensions as RV32I, but removes the top 16 general purpose registers, limiting the core to \texttt{x0} to \texttt{x15}. This can reduce the area used by a RV32 core by up to 25\% when no other extensions are enabled. As our design targets the RV64 architecture, this option is incompatible and disabled.

\subsection{Integer Multiplier}
The integer multiplier unit in RocketCore is customised using a set of parameters set in \texttt{MulDivParams} and passed to the \texttt{MulDiv} class for hardware creation. The parameters for this vary between the design scenarios, as seen in \label[]{}%todo add label

\subsubsection{Unrolling}
The multiplier unit allows for the unrolling of both multiplier and division instructions. Unrolling controls how much of the multiplication operation is completed in a single clock cycle. Without unrolling, multiplier latency is 64 cycles in RV64 architecture, with a single bit computed per cycle. Increasing the unrolling factor decreases the latency in a ratio of \texttt{64/unroll} for the multiplier and \texttt{1 + 64/unroll} for divider. The inverse relation between unroll factor and latency means increasing the factor can provides dramatic decreases for small factors, but diminishing returns are achieved as the unroll factor increases. An issue with increasing the unroll factor is the constraints placed on the clock speed. The increased amount of computation performed each cycle increases the minimum required time period to finish and may violate the setup time if the clock speed is not adjusted accordingly.

Unrolling multiplication and division operations result in unequal changes to timing constraints - each individual division operation takes longer than a multiplication operation. To maintain high clockspeed, division operations cannot be unrolled as much as multiplication operations.

\subsubsection{Early out}
The multiplier unit also has an option for early output of results when possible. A mask is applied to the input and the early output bit is set when the result can be read. Early output can result in a large latency decrease depending on the situation, as small integer multiplications will result in smaller outputs that will likely not require all integer bits and can make use of the early out. The minimum latencies for multiplication and division is reduced to 2 cycles and 3 cycles in a best case scenario.

Granularity for the early out can also be set for the division operations.

\subsection{Floating Point Unit}
The floating point unit (FPU) can also be customised in a limited way, using the \texttt{FPUParams} class passed to the FPU generator. Setting this class to \texttt{None} instead of instantiating it results in no FPU initialised, and the ISA lacks F or D extension support. FPUs are much more complicated than integer operation units, with the RocketCore implementation including subunits for integer to FP conversion and vice-versa, pipelines and register file. Use of the FPU is discussed more in \label[]{}.%todo add label

\subsubsection{Floating point lengths}
The minimum float length (\texttt{minFLen} in the \texttt{FPUParams class}) can be changed to support half-precision floating points of 16 bits, and the maximum float length \texttt{fLen} changed to support double-precision floating points. The FPU in RocketCore does not support quad-precision floating points of 128-bits, though this is not widely used in CPUs.

\subsubsection{FMA Latency}
The RocketCore FPU supports fuse multiply-add operations, where multiplication and addition are performed with a single operation. These are pipelined instructions for RocketCore, and the latency can be specified at instantiation. The cycle latency choice is bounded, allowing 1 to 3 cycles. Similar to the multiplier unrolling, decreased latency can come at the cost of increased clock period, decreasing clock frequency, but may overall be an improvement in FPU performance.

\subsubsection{Squareroot}
The \texttt{divSqrt} option enables a dedicated square root function in the FPU, allowing the square root of numbers to be found faster than regular division. This is a useful function due to the increased use of square roots in modern CAD tools and 3D graphics processing, where slow square root can bottleneck a processor\cite{squareroot-fpu}.

\subsection{Local Interrupts}
Additional local interrupts can be added to the RocketCore using \texttt{nLocalInterrupts} option. These additional interrupts are added to the CSRs and are accessible from all implemented modes. No additional local interrupts are necessary for this design, as there are already multiple available within the standard CSRs.

\subsection{Non-maskable interrupt}
The use of non-maskable interrupts can be disabled in RocketCore, meaning it would technically not meet RISC-V specification when disabled. However, enabling/disabling this option has no functional difference for software as NMI is used exclusively for hardware error conditions and cause an immediate jump to the NMI vector in M-mode to address the issue. NMIs have been disabled in our implementation as we do not foresee a use for them in the project.

\subsection{Hardware Breakpoints}
Hardware breakpoints can be added to the core for use with debuggers with the \texttt{nBreakpoints} option. This adds hardware resources to set breakpoints in running code, using triggers on address/data/memory operations. For testing purposes, we've included a single hardware breakpoint in each core, though actual usage of the hardware breakpoints is not anticipated due to the existence of software breakpoints in the RISC-V specification, as well as the debug module that can already be used to stop and step through sections of programs.

\subsection{Physical Memory Protection}
A custom amount of physical memory protection regions can be defined in RocketCore with \texttt{nPMPs}. PMP is used to limit or increase the physical address space available to software, to increase security and reduce faults. This primarily applies to U and S-mode software, where regions of memory are typically blocked off when using an OS or similar host software and security is required. PMPs can also optionally be configured to apply to M-mode accesses. U and S-mode are not implemented in this project and we will not require security for this design, but future programs utilising the designs might. We elected to include 8 PMP regions, as this was the amount in a default big core.

\subsection{Performance Counters} %todo this

\subsection{Cache flush}
RocketCore implements a custom instruction for fully flushing the data cache, enabled with \texttt{haveCFlush}. The instruction has been disabled for our designs.

\subsection{Writable ISA}
RocketCore allows writing to the register holding ISA details, which is automatically configured and written to in the design. This has been disabled for our designs as the automatic ISA is accurate.

\subsection{L2 TLB}
An L2 TLB can be configured on-CPU in RocketCore. Due to the size constraints on our designs, we have not implemented this for our cores.

\subsection{Page Table Entry Cache}
We've configured our designs with an 8 entry PTE cache. Code being run on the design will be very small and likely not span more than a single page, but the design should be suitable for future programs that may take more space, and so would benefit from a larger PTE cache.

\subsection{Machine Trap Vector}
The machine trap vector is configurable in RocketCore. The startup value can be set, as well as preventing the trap vector from being modified. Our designs init with a trap vector of \texttt{0x0} and the trap vector is writable - the boot loader sets the trap vector on startup, so setting a default value is not required.

\subsection{Fast loading}
RocketCore allows data to be loaded directly to the core when fetching from memory when enabled via the \texttt{fastLoadWord} and \texttt{fastLoadByte} options. Typically, a cache miss occurs and data is fetched from memory into cache, and then loaded from cache to the core. Enabling these options add bypasses, allowing data to be loaded directly from memory into the core. Having both of these options enabled is allowed, but only one is ever used - \texttt{loadFastByte} takes priority during compilation to RTL. Our designs use \texttt{loadFastWord}, allowing a full word to bypass the cache instead of just the first byte.

\subsection{Branch Prediction CSR}
RocketCore has the option to add a custom branch prediction CSR. This contains two bits, used for requesting to completely flush the branch target buffer and to set the branch prediction CSR to static. We disabled this option in our design.,

\subsection{Clock Gating} %todo this

\subsection{Instruction Cache (L1)}
The instruction cache on the core is customised using the \texttt{ICacheParams} class.

\begin{description}
    \item[\texttt{nSets}] Amount of sets in the cache
    \item[\texttt{nWays}] Amount of ways in each set
    \item[\texttt{rowBits}] Appears to have no function in \texttt{ICache}
    \item[\texttt{nTLBSets}] Amount of sets in the cache TLB
    \item[\texttt{nTLBWays}] Amount of ways in each TLB set
    \item[\texttt{nTLBBasePageSectors}] How many sectors to divide each page into for easier memory loading
    \item[\texttt{nTLBSuperpages}] Amount of superpages - much larger page than normal, providing larger TLB coverage
    \item[\texttt{tagECC}] ECC for tags in cache
    \item[\texttt{dataECC}] ECC for cache data
    \item[\texttt{itimAddr}] Instruction Tightly Integrated Memory base address and enables ITIM if not \texttt{None}
    \item[\texttt{prefetch}] Enables pre-fetching, getting next cache line in advance
    \item[\texttt{blockBytes}] Size of each cache line
    \item[\texttt{latency}] Latency of a fetch instruction, 1 or 2 cycles. Decreased latency limits clock frequency
    \item[\texttt{fetchBytes}] Bytes fetched by CPU for each cycle
\end{description}

\subsection{Data Cache (L1)}
The data cache on the core is customised using the \texttt{DCacheParams} class.

\begin{description}
    \item[\texttt{nSets}] Amount of sets in the cache
    \item[\texttt{nWays}] Amount of ways in each set
    \item[\texttt{rowBits}] Appears to have no function in \texttt{DCache}
    \item[\texttt{subWordBits}] Amount of bits in each subword in the cache word
    \item[\texttt{replacementPolicy}] Replacement policy - valid options are random, least recently used and pseudo least recently used
    \item[\texttt{nTLBSets}] Amount of sets in the cache TLB
    \item[\texttt{nTLBWays}] Amount of ways in each TLB set
    \item[\texttt{nTLBBasePageSectors}] How many sectors to divide each page into for easier memory loading
    \item[\texttt{nTLBSuperpages}] Amount of superpages - much larger page than normal, providing larger TLB coverage
    \item[\texttt{tagECC}] ECC for tags in cache
    \item[\texttt{dataECC}] ECC for cache data
    \item[\texttt{dataECCBytes}] Bytes used for the tagECC
    \item[\texttt{nMSHRs}] Number of miss status holding registers, tracking outstanding cache misses
    \item[\texttt{nSDQ}] Store Data Queue buffers storing data from execute unit
    \item[\texttt{nRPQ}] Queue config option for MSHRs
    \item[\texttt{nMMIOs}] Memory mapped IO cache entries
    \item[\texttt{blockBytes}] Bytes per data cache line
    \item[\texttt{clockGate}] Enable clock gating in the data cache
    \item[\texttt{scratch}] Enable scratchpad in data cache
\end{description}

\subsection{Branch Target Buffer and Branch History Table}
A BTB and BHT can also be created by RocketCore. The BTB is a fully-associative cache, mapping instruction addresses to predicted PC contents. The RockerCore implementation has parameterisable amount of entries, match bits, pages, return address stacks and out-of-order updates.

\begin{description}
    \item[\texttt{nEntries}] BTB entries
    \item[\texttt{nMatchBits}] Instruction address matching bits
    \item[\texttt{nPages}] BTB pages
    \item[\texttt{nRAS}] Number of return address stacks
    \item[\texttt{updatesOutOfOrder}] Enable updating BTB out of order
\end{description}

The BTB contains the BHT, tracking the history of whether a branch is taken or not. The BHT has a parameterisable amount of entries, counter length (1 or 2 bits), history length and history bits.

\begin{description}
    \item[\texttt{nEntries}] Pattern entries in the BHT
    \item[\texttt{Counter length}] Number of prediction bits
    \item[\texttt{historyLength}] History length
    \item[\texttt{historyBits}] Number of bits in each history entry
\end{description}

\subsection{Default RocketCore CPUs}
RocketCore contains some predefined core designs. These allow a user to immediately generate a RocketCore CPU without having to parameterise their own, with a few different options dependant on the use case.

\subsubsection{Big Core}
The default big core with RocketCore

\subsubsection{Small Core}
The RocketCore predefined small core implements the RV64IMAZicsrZifenceiC ISA, removing the single-precision floating point and double-precision floating point instructions from the RV64GC ISA. Removing the FPU from the core much reduces the total logic taken by the core, reducing 

\begin{figure}
    \includegraphics[]{./img/rocketcore_default_small.png}
\end{figure}

The MMU has also been removed from the CPU, preventing virtual memory from being used. The removal of virtual memory allows programs to interfere with each others memory, and this can be very difficult to deal with if programs are dynamic in memory and the amount of memory they consume. The main issues stemming from this are lack of security and instability. Virtual memory invalidates read attempts to memory spaces not under the control of that process, and no virtual memory allows a malicious process to read any other processes memory, including any sensitive data they might hold. Virtual memory also prevents write attempts to other processes memory space, and so a process might overwrite another's memory during runtime, leading to one process, both processes or the entire system stopping unexpectedly.

The core has separate data and instructions caches, each with 64 sets of 1 ways, giving 64 lines of 8 bytes and a total of 512 bytes. This is a very small, and any computation that uses many pieces of data will be very memory bound.

A small BTB (Branch Target Buffer) with 28 entries and a BHT (Branch History Table). The BTB stores the destination of recent branches, with the BHT the history of whether the branch was taken. This forms a branch predictor, that attempts to predict if a branch is taken and if so, where it will jump to. This is a relatively small branch predictor, and would suffer in complex programs with many varying branches.

The multiplier divider unit is small, without unrolling or early out. 

\subsection{Final Design Scenario Limitations}
Both custom cores implement the RV64IMAZicsrZifenceiC ISA, with MMU. Experimentation with FPUs was attempted, and different cores successfully synthesised and implemented on the FPGA with various FPU configurations. However, attempting to synthesis a dual core design was unsuccessful - the amount of LUTs required for such a design was always greater than the amount available on the FPGA, no matter the reductions made in other options like cache.

Cores with different ISAs were considered as a solution to this approach. We first verified that this was possible, and successfully implemented a dual core SoC with a large FPU-enabled core and a small FPU-disabled core. While a functional hardware design, the software design for this SoC would be much harder due to the differing ISAs. Running the Linux kernel would also be more complicated: a custom kernel can be compiled to use soft-float (running floating point operations using integer ALU and registers) instead of FPU hardware, this would prevent the FPU actually being used by Linux and programs run within the OS, thus rendering the FPU in the big core pointless.

Another option considered was to share the FPU between cores, but research into this showed RocketChip is not capable of such designs. As such, the final design has not been able to include an FPU and we dropped support for the F and D ISA extensions.

%todo include FPU resource comparisons here

\subsection{Final Design Scenario - Custom Big Core}
\subsubsection{Multiplier} %todo include code listing and relevant design screenshots
The multiplier in the big core is unrolled by a factor of 16. With a 

\subsection{Final Design Scenario - Custom Small Core}


\chapter{Test Design}
\label{ch:test_design}

\section{Software Verification}
Software verification was completed using the Spike RISC-V emulator. Spike is a functional model of a RISC-V processor, with options for number of harts (hardware threads, RISC-V representation of logical cores), and ISA. Spike supports all major ratified extensions, as well as some still in the proposed stage. As our final hardware implementation uses RV64IMAZicsrZifenceiC, we can set Spike to simulate the same core type when invoking from the command line.

